{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing our Data for Fine-Tuning / or Text Generation / Stimuli\n",
    "This file covers how a function was created that could shorten articles based on the requirements mentioned in the thesis. It also includes how these articles then could be converted into the JSONL format which was necessary for fine-tuning GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordDetokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to shorten articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_article(article: str) -> str:\n",
    "    '''\n",
    "    Function to shorten an article to roughly 120 words using nltk (stops at the first natural punctuation after 120 words) \n",
    "\n",
    "    Parameters\n",
    "    article (text to be shortened): str\n",
    "    '''\n",
    "\n",
    "    tokens = word_tokenize(article) #use nltk to tokenize the text\n",
    "    punctuations = [\".\", \"!\", \"?\", \",\"]\n",
    "    stops = [\".\", \"!\", \"?\",]\n",
    "\n",
    "    counter = 0 \n",
    "    \n",
    "    short_article = []\n",
    "    \n",
    "    for i in range(len(tokens)): \n",
    "        if counter > 120 and tokens[i] in stops: # if counter is over 120 & token is a stop, break the loop (to not cut in middle of a sentence)\n",
    "            short_article.append(tokens[i]) \n",
    "            break\n",
    "        else: \n",
    "            if tokens[i] not in punctuations: # count the number of words (not punctuations)\n",
    "                short_article.append(tokens[i])\n",
    "                counter += 1\n",
    "            else: \n",
    "                short_article.append(tokens[i]) # add punctuation to the list but not count it\n",
    "\n",
    "    detokenize_article = TreebankWordDetokenizer().detokenize(short_article) # use nltk to detokenize the text\n",
    "    detokenize_article = detokenize_article.replace(\" .\", \".\") # fix detokenization errors\n",
    "\n",
    "    return detokenize_article\n",
    "\n",
    "def shorten_all_articles(text_column:pd.Series)-> list:\n",
    "    '''\n",
    "    Function to shorten all articles in a text column using the shorten_article function\n",
    "\n",
    "    Parameters\n",
    "    text_column (column with articles to be shortened): pd.Series\n",
    "    '''\n",
    "    short_articles = []\n",
    "    for article in text_column:\n",
    "        short_articles.append(shorten_article(article))\n",
    "    return short_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert short articles to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_article_data(data, header_col, sub_header_col, short_text_col, output_name):\n",
    "    ''' \n",
    "    Function to create a jsonl file with the following structure:\n",
    "    prompt: <header> <subheader>\n",
    "    completion: <short_text>\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas dataframe): dataframe with the data\n",
    "    header_col (str): name of the column with the header\n",
    "    sub_header_col (str): name of the column with the subheader\n",
    "    short_text_col (str): name of the column with the shortened text\n",
    "    output_name (str): name of the output file\n",
    "    '''\n",
    "\n",
    "    # create prompt column\n",
    "    data[\"prompt\"] = data[header_col].astype(str) +\". \"+ data[sub_header_col]\n",
    "\n",
    "    # rename short_text to completion\n",
    "    data = data.rename(columns={short_text_col: \"completion\"})\n",
    "\n",
    "    # rearrange columns for jsonl format\n",
    "    data = data[[\"prompt\", \"completion\"]]\n",
    "\n",
    "\n",
    "    # prepare for jsonl fine-tuning (prompts ending with \"->\" and completitons starting with white space \" \" & ending with \"\\n\")\n",
    "    data[\"prompt\"] = data[\"prompt\"].astype(str) + \" ->\"\n",
    "    data[\"completion\"] = \" \"+ data[\"completion\"].astype(str) + \" \\n\"\n",
    "\n",
    "    short_json = data.to_json(f\"{output_name}.jsonl\", orient='records', lines=True, force_ascii=False) #needs force_ascii=False to keep danish characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"~/Desktop/politics-GPT3/data/articles (20. sep - 30. oct)/final_articles (20. sep - 30. oct).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    En ny geopolitisk orden i Europa. Sammenstød m...\n",
       "1    Græsset står højt ved Per Lauritsens hus i Aal...\n",
       "2    Elena Smith er 71 år og aktivist. Da hun var y...\n",
       "3    Jesper Jarlbæk viser rundt på Christianshavn, ...\n",
       "4    Tidligere summede burene i de store haller af ...\n",
       "5    Da den tidligere ejer af Hesbjerg Slot Jørgen ...\n",
       "6    Der venter en historisk stor begivenhed, når v...\n",
       "7    En kvinde i 60'erne er søndag formiddag afgået...\n",
       "8    I 2023 har 19 kommuner fået mulighed for enten...\n",
       "9    Niels Laursen kan se spidsen af ti vindmøller ...\n",
       "Name: short_text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"short_text\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"shuffled_final_articles (20. sep - 30. oct).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the functions on our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samf_data = pd.read_csv(\"data/nyheder_samfund_only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samf_data[\"short_text\"] = shorten_all_articles(samf_data['tekst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Titusindvis af ansøgere får snart besked om, hvilken videregående uddannelse de er kommet ind på - og dermed også om deres karaktergennemsnit er over adgangskvotienten. Men mange kommer ikke ind på en af deres ønskede uddannelser og skal i stedet kigge sig om efter andre muligheder. Til dem (og andre nysgerrige) har vi lave denne side, der kan fortælle, hvor mange og hvilke uddannelser snittet rækker til - hvis man ellers opfylder de andre krav til for eksempel særlige fag. Træk i håndtaget for at se, hvor mange uddannelser dit karaktergennemsnit giver adgang til:, Er dit eksamensbevis med karakterer fra den gamle 13-skala, kan du se her, hvordan du omregner den til den nye syvtalsskala. Listen rummer kun de videregående uddannelser, der er en del af Den Koordinerede Tilmelding (KOT).'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samf_data[\"short_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_article_data(samf_data, \"headers\", \"sub_header\", \"short_text\", \"samf_articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
